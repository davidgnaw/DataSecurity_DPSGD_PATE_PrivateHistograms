{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb1bde7-aaa3-4dfe-89ac-0e0109a8627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a278d501-467b-4f28-b249-dbd43a4d8d29",
   "metadata": {},
   "source": [
    "# DP-SGD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18254f5c-164d-47fe-a50c-e5627d609215",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent: optimizer for neural network \n",
    "- iterative procedure where at each iteration, batch of data randomly sampled from training set\n",
    "- error b/w model's prediction and training labels computed (error called loss) and this loss is then differentiated with respect to model's parameters\n",
    "- the derivatives (graidents) tells us how to update each parameter to bring model closer to predicting correct label\n",
    "- iterativcely recomputing gradients and applying them to update model's parameter is called descent\n",
    "- algorithm works by... \n",
    "1) sampling minibatch of training points (x, y) with x as input and y as label\n",
    "2) compute loss (error) by L(theta, x, y) b/w model's prediction f_theta(x) and label y where theta represents model parameters\n",
    "3) compute gradient of loss L(theta, x, y) with respect to model parameters theta\n",
    "4) finally multiply gradients by learning rate and apply product to update model parameters theta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de433c8-b412-4a53-9401-525fca21c253",
   "metadata": {},
   "source": [
    "Making Stochastic Gradient Descent Differentially Private\n",
    "- make 2 modifications to make DP-SGD, first, sensitivity of each gradient needs to be bounded (limit how much each individual training point sampled in minibatch can influence resulting gradeint computation) by clipping each gradient between steps 3 and 4. Second, randomize algorithm's behavior to make it statistically impossible to know if a particular point was included by comparing updates stochastic gradient descent applies when it operates with or without a certain point and is done by sampling random noise and adding it to clipped gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7024333-29aa-4c83-8828-d431e77ea7f6",
   "metadata": {},
   "source": [
    "Importance of TF Privacy\n",
    "- TF Privacy provides code to wrap existing TF optimizer to create variant that performs steps needed to make differentially private SGD\n",
    "- Code...\n",
    "- to compute loss between model's prediction and labels -> vector_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits) -> we are using cross-entropy as common loss because it is well-suited for the classification problem (MNIST)\n",
    "- to create an optimizer in TF, instantiate it by passing learning rate value with optimizers.dp_optimizer module of TF privacy to implement DF -> optimizer = optimizers.dp_optimizer.DPGradientDescentGaussianOptimizer(\n",
    "    l2_norm_clip=FLAGS.l2_norm_clip,\n",
    "    noise_multiplier=FLAGS.noise_multiplier,\n",
    "    num_microbatches=FLAGS.microbatches,\n",
    "    learning_rate=FLAGS.learning_rate,\n",
    "    population_size=60000) \n",
    "    train_op = optimizer.minimize(loss=vector_loss)\n",
    "    \n",
    "- note: TF Privacy introduces 3 new hyperparameters to optimizer object including l2_norm clip (maximum Euclidean norm of each individual gradient to bound optimizer's sensitivity to individual training points), noise_multipler/Sigma (control how much noise is sampled and added to gradients before applied by optimizer and more noise results in better privacy at expense of lower utility) and num_microbatches (clipping by microbatches to allow for parallelism)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fbce59b-7178-4587-a29e-d8722af21842",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'app'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xw/gjkqcy4x1gqgcyvc45ys8srw0000gn/T/ipykernel_6544/4254463897.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mflags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'app'"
     ]
    }
   ],
   "source": [
    "# define flags\n",
    "\n",
    "flags.DEFINE_boolean(\n",
    "    'dpsgd', True, 'If True, train with DP-SGD. If False, '\n",
    "    'train with vanilla SGD.')\n",
    "flags.DEFINE_float('learning_rate', 0.15, 'Learning rate for training')\n",
    "flags.DEFINE_float('noise_multiplier', 0.1,\n",
    "                   'Ratio of the standard deviation to the clipping norm')\n",
    "flags.DEFINE_float('l2_norm_clip', 1.0, 'Clipping norm')\n",
    "flags.DEFINE_integer('batch_size', 250, 'Batch size')\n",
    "flags.DEFINE_integer('epochs', 60, 'Number of epochs')\n",
    "flags.DEFINE_integer(\n",
    "    'microbatches', 250, 'Number of microbatches '\n",
    "    '(must evenly divide batch_size)')\n",
    "flags.DEFINE_string('model_dir', None, 'Model directory')\n",
    "\n",
    "FLAGS = flags.FLAGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ba7372c-2625-4f38-8045-8d389eca89d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnparsedFlagAccessError",
     "evalue": "Trying to access flag --dpsgd before flags were parsed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnparsedFlagAccessError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xw/gjkqcy4x1gqgcyvc45ys8srw0000gn/T/ipykernel_6514/741247644.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpsgd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmicrobatches\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of microbatches should divide evenly batch_size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;31m# get too much noise.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnparsedFlagAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnparsedFlagAccessError\u001b[0m: Trying to access flag --dpsgd before flags were parsed."
     ]
    }
   ],
   "source": [
    "def compute_epsilon(steps):\n",
    "\n",
    "    if FLAGS.noise_multiplier == 0.0:\n",
    "        return float('inf')\n",
    "    orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n",
    "    sampling_probability = FLAGS.batch_size / 60000\n",
    "    rdp = compute_rdp(\n",
    "      q=sampling_probability,\n",
    "      noise_multiplier=FLAGS.noise_multiplier,\n",
    "      steps=steps,\n",
    "      orders=orders)\n",
    "    # Delta is set to 1e-5 because MNIST has 60000 training points.\n",
    "    return get_privacy_spent(orders, rdp, target_delta=1e-5)[0]\n",
    "\n",
    "\n",
    "def load_mnist():\n",
    "  \n",
    "    train, test = tf.keras.datasets.mnist.load_data()\n",
    "    train_data, train_labels = train\n",
    "    test_data, test_labels = test\n",
    "\n",
    "    train_data = np.array(train_data, dtype=np.float32) / 255\n",
    "    test_data = np.array(test_data, dtype=np.float32) / 255\n",
    "\n",
    "    train_data = train_data.reshape((train_data.shape[0], 28, 28, 1))\n",
    "    test_data = test_data.reshape((test_data.shape[0], 28, 28, 1))\n",
    "\n",
    "    train_labels = np.array(train_labels, dtype=np.int32)\n",
    "    test_labels = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "    train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
    "    test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "    assert train_data.min() == 0.\n",
    "    assert train_data.max() == 1.\n",
    "    assert test_data.min() == 0.\n",
    "    assert test_data.max() == 1.\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "    \n",
    "    \n",
    "logging.set_verbosity(logging.INFO)\n",
    "if FLAGS.dpsgd and FLAGS.batch_size % FLAGS.microbatches != 0:\n",
    "    raise ValueError('Number of microbatches should divide evenly batch_size')\n",
    "\n",
    "# Load training and test data.\n",
    "train_data, train_labels, test_data, test_labels = load_mnist()\n",
    "\n",
    "# Define a sequential Keras model\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Conv2D(\n",
    "      16,\n",
    "      8,\n",
    "      strides=2,\n",
    "      padding='same',\n",
    "      activation='relu',\n",
    "      input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPool2D(2, 1),\n",
    "  tf.keras.layers.Conv2D(\n",
    "      32, 4, strides=2, padding='valid', activation='relu'),\n",
    "  tf.keras.layers.MaxPool2D(2, 1),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "if FLAGS.dpsgd:\n",
    "    optimizer = DPKerasSGDOptimizer(\n",
    "        l2_norm_clip=FLAGS.l2_norm_clip,\n",
    "        noise_multiplier=FLAGS.noise_multiplier,\n",
    "        num_microbatches=FLAGS.microbatches,\n",
    "        learning_rate=FLAGS.learning_rate)\n",
    "    # Compute vector of per-example loss rather than its mean over a minibatch.\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=FLAGS.learning_rate)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile model with Keras\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Train model with Keras\n",
    "model.fit(\n",
    "  train_data,\n",
    "  train_labels,\n",
    "  epochs=FLAGS.epochs,\n",
    "  validation_data=(test_data, test_labels),\n",
    "  batch_size=FLAGS.batch_size)\n",
    "\n",
    "# Compute the privacy budget expended.\n",
    "if FLAGS.dpsgd:\n",
    "    eps = compute_epsilon(FLAGS.epochs * 60000 // FLAGS.batch_size)\n",
    "    print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
    "else:\n",
    "    print('Trained with vanilla non-private SGD optimizer')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928ad0b2-f78c-4b64-a82f-22049b5f2b87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
